# Submind System Configuration
# LM Studio local server - Make sure LM Studio is running at http://192.168.4.30:1234

# Global Settings
conversation:
  max_rounds: 4                    # Maximum number of discussion rounds (increased for more interaction)
  detect_consensus: true           # Enable smart termination detection
  consensus_threshold: 0.7         # Similarity threshold for detecting repetition
  minimum_responses_per_submind: 1 # Each submind must respond at least this many times
  delay_between_subminds: 1.5      # Seconds to wait between each submind (simulate reading time)
  enable_export: true              # Auto-export conversations after completion

# Default Model (used if submind-specific model not set)
# Currently using local LM Studio model
default_model: "mistralai/mistral-7b-instruct-v0.3"

# Submind Configurations
# All subminds use the same local LM Studio model
# Different temperatures create varied perspectives from the same model

subminds:
  - name: "Doctrinal"
    role: "traditional"
    model: "mistralai/mistral-7b-instruct-v0.3"
    temperature: 0.7
    max_tokens: 500
    color: "blue"

  - name: "Analytical"
    role: "analytical"
    model: "mistralai/mistral-7b-instruct-v0.3"
    temperature: 0.5  # Lower temp for more focused analysis
    max_tokens: 500
    color: "cyan"

  - name: "Strategic"
    role: "strategic"
    model: "mistralai/mistral-7b-instruct-v0.3"
    temperature: 0.6
    max_tokens: 500
    color: "green"

  - name: "Creative"
    role: "creative"
    model: "mistralai/mistral-7b-instruct-v0.3"
    temperature: 0.9  # Higher temp for more creative responses
    max_tokens: 500
    color: "magenta"

  - name: "Skeptic"
    role: "skeptic"
    model: "mistralai/mistral-7b-instruct-v0.3"
    temperature: 0.6
    max_tokens: 500
    color: "red"

# Export Settings
export:
  format: ["json", "markdown"]  # Export formats to generate
  directory: "exports"
  include_metadata: true
  include_timestamps: true

# LM Studio Configuration Notes:
# - Make sure LM Studio is running and has a model loaded
# - Server should be accessible at http://192.168.4.30:1234
# - You can change the server URL in .env using LMSTUDIO_BASE_URL
# - Only one model at a time is supported per LM Studio instance

# Submind Presets
# Define preset combinations for different use cases
submind_presets:
  all:
    name: "All Minds"
    description: "Full discussion with all perspectives"
    subminds: [Doctrinal, Analytical, Strategic, Creative, Skeptic]

  science:
    name: "Science Focus"
    description: "Analytical and empirical discussion"
    subminds: [Analytical, Skeptic, Creative]

  politics:
    name: "Politics & Policy"
    description: "Strategic and doctrinal analysis"
    subminds: [Doctrinal, Strategic, Skeptic]

  creative:
    name: "Creative Brainstorm"
    description: "Innovative thinking and analysis"
    subminds: [Creative, Analytical, Strategic]

  quick:
    name: "Quick Chat"
    description: "Fast discussion with 2 minds"
    subminds: [Analytical, Strategic]
